{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbBABxu5m5HCUKFko3edE8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phoumithona/matrix_computation/blob/master/Assignment_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s4hDgXh1yTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "file_data   = \"mnist.csv\"\n",
        "handle_file = open(file_data, \"r\")\n",
        "data        = handle_file.readlines()\n",
        "handle_file.close()\n",
        "\n",
        "size_row    = 28    # height of the image\n",
        "size_col    = 28    # width of the image\n",
        "\n",
        "num_image   = len(data)\n",
        "count       = 0     # count for the number of images\n",
        "\n",
        "# normalize the values of the input data to be [0, 1]\n",
        "def normalize(data):\n",
        "    data_normalized = (data - min(data)) / (max(data) - min(data))\n",
        "    return(data_normalized)\n",
        "\n",
        "# make a matrix each column of which represents an images in a vector form\n",
        "list_image  = np.empty((size_row * size_col, num_image), dtype=float)\n",
        "list_label  = np.empty(num_image, dtype=int)\n",
        "\n",
        "for line in data:\n",
        "    line_data   = line.split(',')\n",
        "    label       = line_data[0]\n",
        "    im_vector   = np.asfarray(line_data[1:])\n",
        "    im_vector   = normalize(im_vector)\n",
        "\n",
        "    list_label[count]     = label\n",
        "    list_image[:, count]  = im_vector\n",
        "\n",
        "    count += 1\n",
        "\n",
        "# split dataset\n",
        "x_train = list_image[:,:1000] # 6000 rows training image\n",
        "y_train = list_label[:1000,]  # 6000 rows training label\n",
        "x_test  = list_image[:,1000:] # 4000 rows testing image\n",
        "y_test  = list_label[1000:,]  # 4000 rows testing label\n",
        "\n",
        "# data transpose\n",
        "x_train = x_train.T\n",
        "y_train = y_train\n",
        "x_test  = x_test.T\n",
        "y_test  = y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn78z9UNU3Ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "#\n",
        "# build our neural network model\n",
        "#\n",
        "class NN:\n",
        "    first_layer = {}\n",
        "    second_layer = {}\n",
        "\n",
        "    def __init__(self, inputs, hidden, outputs):\n",
        "        # initialize the model parameters, including the first and second layer \n",
        "        # parameters and biases\n",
        "        self.first_layer['para'] = np.random.randn(hidden,inputs) / np.sqrt(num_inputs)\n",
        "        self.first_layer['bias'] = np.random.randn(hidden,1) / np.sqrt(hidden)\n",
        "        self.second_layer['para'] = np.random.randn(outputs,hidden) / np.sqrt(hidden)\n",
        "        self.second_layer['bias'] = np.random.randn(outputs,1) / np.sqrt(hidden)\n",
        "        self.input_size = inputs\n",
        "        self.hid_size = hidden\n",
        "        self.output_size = outputs\n",
        "\n",
        "    def __activfunc(self, Z, type = 'sigmoid', deri = False):\n",
        "        # implement the activation function\n",
        "        if type == 'sigmoid':\n",
        "            if deri == True:\n",
        "                return 1/(1 + np.exp(-Z))*(1-1/(1 + np.exp(-Z)))\n",
        "            else:\n",
        "                return 1/(1 + np.exp(-Z))\n",
        "        else:\n",
        "            raise TypeError('Invalid type!')\n",
        "\n",
        "    def __cross_entropy_error(self,v,y):\n",
        "        # implement the cross entropy error\n",
        "        return -np.log(v[y])\n",
        "\n",
        "    def __forward(self,x,y):\n",
        "        # implement the forward computation, calculation of prediction list and error\n",
        "        Z = np.matmul(self.first_layer['para'],x).reshape((self.hid_size,1)) + self.first_layer['bias']\n",
        "        H = np.array(self.__activfunc(Z)).reshape((self.hid_size,1))\n",
        "        U = np.matmul(self.second_layer['para'],H).reshape((self.output_size,1)) + self.second_layer['bias']\n",
        "        predict_list = np.squeeze(self.__activfunc(U))\n",
        "        error = self.__cross_entropy_error(predict_list,y)\n",
        "        \n",
        "        dic = {\n",
        "            'Z':Z,\n",
        "            'H':H,\n",
        "            'U':U,\n",
        "            'f_X':predict_list.reshape((1,self.output_size)),\n",
        "            'error':error\n",
        "        }\n",
        "        return dic\n",
        "\n",
        "    def __back_propagation(self,x,y,f_result):\n",
        "        # implement the back propagation process, compute the gradients\n",
        "        E = np.array([0]*self.output_size).reshape((1,self.output_size))\n",
        "        E[0][y] = 1\n",
        "        dU = (-(E - f_result['f_X'])).reshape((self.output_size,1))\n",
        "        db_2 = copy.copy(dU)\n",
        "        dC = np.matmul(dU,f_result['H'].transpose())\n",
        "        delta = np.matmul(self.second_layer['para'].transpose(),dU)\n",
        "        db_1 = delta.reshape(self.hid_size,1)*self.__activfunc(f_result['Z'], \n",
        "                                                               deri=True).reshape(self.hid_size,1)\n",
        "        dW = np.matmul(db_1.reshape((self.hid_size,1)),x.reshape((1,784)))\n",
        "\n",
        "        grad = {\n",
        "            'dC':dC,\n",
        "            'db_2':db_2,\n",
        "            'db_1':db_1,\n",
        "            'dW':dW\n",
        "        }\n",
        "        return grad\n",
        "\n",
        "    def __optimize(self,b_result, learning_rate):\n",
        "        # update the hyperparameters\n",
        "        self.second_layer['para'] -= learning_rate*b_result['dC']\n",
        "        self.second_layer['bias'] -= learning_rate*b_result['db_2']\n",
        "        self.first_layer['bias'] -= learning_rate*b_result['db_1']\n",
        "        self.first_layer['para'] -= learning_rate*b_result['dW']\n",
        "\n",
        "    def __loss(self,X_train,Y_train):\n",
        "        # implement the loss function of the training set\n",
        "        loss = 0\n",
        "        for n in range(len(X_train)):\n",
        "            y = Y_train[n]\n",
        "            x = X_train[n][:]\n",
        "            loss += self.__forward(x,y)['error']\n",
        "        return loss\n",
        "\n",
        "    def train(self, X_train, Y_train, num_iterations = 1000, learning_rate = 0.5):\n",
        "        # generate a random list of indices for the training set\n",
        "        rand_indices = np.random.choice(len(X_train), num_iterations, replace=True)\n",
        "        \n",
        "        def l_rate(base_rate, ite, num_iterations, schedule = False):\n",
        "        # determine whether to use the learning schedule\n",
        "            if schedule == True:\n",
        "                return base_rate * 10 ** (-np.floor(ite/num_iterations*5))\n",
        "            else:\n",
        "                return base_rate\n",
        "\n",
        "        count = 1\n",
        "        loss_dict = {}\n",
        "        test_dict = {}\n",
        "\n",
        "        for i in rand_indices:\n",
        "            f_result = self.__forward(X_train[i],Y_train[i])\n",
        "            b_result = self.__back_propagation(X_train[i],Y_train[i],f_result)\n",
        "            self.__optimize(b_result,l_rate(learning_rate,i,num_iterations,True))\n",
        "            \n",
        "            if count % 1000 == 0:\n",
        "                if count % 5000 == 0:\n",
        "                    loss = self.__loss(X_train,Y_train)\n",
        "                    test = self.testing(x_test,y_test)\n",
        "                    print('Trained for {} times,'.format(count),\n",
        "                          'Training Loss = {}, Accuarcy Test: = {}'.format(loss, test))\n",
        "                    loss_dict[str(count)] = loss\n",
        "                    test_dict[str(count)] = test\n",
        "                else:\n",
        "                    print('Trained for {} times,'.format(count))\n",
        "            count += 1\n",
        "\n",
        "        print('Training finished!')\n",
        "        return loss_dict, test_dict\n",
        "\n",
        "    def testing(self,X_test, Y_test):\n",
        "        # test the model on the training dataset\n",
        "        total_correct = 0\n",
        "        for n in range(len(X_test)):\n",
        "            y = Y_test[n]\n",
        "            x = X_test[n][:]\n",
        "            prediction = np.argmax(self.__forward(x,y)['f_X'])\n",
        "            if (prediction == y):\n",
        "                total_correct += 1\n",
        "        #print('Accuarcy Test: ',total_correct/len(X_test))\n",
        "        return total_correct/np.float(len(X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}